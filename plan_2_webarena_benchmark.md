# Plan 2: WebArena Verified Benchmark Integration

## Goal

Integrate [WebArena Verified](https://github.com/ServiceNow/webarena-verified) (NeurIPS 2025) as an external benchmark to produce **externally-comparable, deterministic, publication-grade** performance numbers for browser-use.

812 verified tasks across 6 self-hosted web applications. Deterministic evaluation via structured agent responses + HAR network traces. No LLM-as-judge. Template-level macro averaging with 95% bootstrap confidence intervals.

**Why WebArena Verified specifically** (from REPORT_TEST.md §8 reuse assessment):
- `pip install webarena-verified` — lowest integration friction
- Evaluator is **decoupled**: takes `(agent_response.json, network_trace.har)` → score. No browser ownership conflict with browser-use's CDP architecture.
- 812 verified tasks with ground-truth answers — no evaluation ambiguity
- Deterministic scoring with type/normalization-aware comparators — no LLM judge variance
- Self-hosted Docker environments — fully reproducible, no live URL dependency

---

## Architecture

```
benchmarks/
├── webarena/
│   ├── __init__.py
│   ├── adapter.py         # Maps browser-use Agent output → WebArena response format
│   ├── runner.py           # Runs tasks: Agent drives browser, collects HAR + response
│   ├── evaluator.py        # Wraps webarena-verified evaluation API
│   ├── report.py           # WebArena-specific reporting (per-site, macro-averaged)
│   ├── config.py           # Environment URLs, credentials, container management
│   └── auth.py             # CDP-based authentication (cookie injection)
├── metrics.py              # Shared with Plan 1
├── baseline.py             # Shared with Plan 1
└── reports/
    └── .gitkeep
```

---

## Prerequisites

### WebArena Verified Package
```bash
uv add webarena-verified
```

### Docker Containers (6 self-hosted web apps)

| Site | Web Port | env-ctrl Port | Image |
|------|----------|---------------|-------|
| Shopping (Magento) | 7770 | 7771 | `am1n3e/webarena-verified-shopping` |
| Shopping Admin | 7780 | 7781 | `am1n3e/webarena-verified-shopping_admin` |
| Reddit (Lemmy) | 9999 | 9998 | `am1n3e/webarena-verified-reddit` |
| GitLab | 8023 | 8024 | `am1n3e/webarena-verified-gitlab` |
| Wikipedia (Kiwix) | 8888 | — | Via `webarena-verified env setup init --site wikipedia` |
| Map (OSM) | 3000 | — | Single container |

**Resource requirements**: 4 vCPUs, 16 GB RAM minimum. GitLab alone consumes 2.5–6+ GB.

**Startup via CLI:**
```bash
webarena-verified env start --site shopping
webarena-verified env start --site reddit
# ... etc, or start all:
webarena-verified env start --all
```

**Environment reset** (between runs for state-changing tasks):
```bash
curl -X POST http://localhost:7771/init   # shopping
curl -X POST http://localhost:9998/init   # reddit
```

---

## Step 1: `benchmarks/webarena/config.py` — Environment Configuration

A pydantic model holding container URLs and credentials:

```python
class WebArenaEnvironment(BaseModel):
    model_config = ConfigDict(extra='forbid')

    shopping_url: str = "http://localhost:7770"
    shopping_admin_url: str = "http://localhost:7780"
    reddit_url: str = "http://localhost:9999"
    gitlab_url: str = "http://localhost:8023"
    wikipedia_url: str = "http://localhost:8888"
    map_url: str = "http://localhost:3000"

    # Default credentials from WebArena docs
    shopping_credentials: tuple[str, str] = ("emma.lopez@gmail.com", "Password.1")
    gitlab_credentials: tuple[str, str] = ("root", "demopass")
    reddit_credentials: tuple[str, str] = ("MarvelsGrantMan136", "test1234")
```

**URL placeholder resolution**: WebArena tasks use `__SHOPPING__`, `__GITLAB__`, etc. in `start_url`. This config resolves them to actual container URLs.

**Health check**: `check_environments()` method that hits each container's web port and returns which sites are up.

---

## Step 2: `benchmarks/webarena/auth.py` — CDP Cookie Authentication

WebArena tasks often require authenticated sessions. The original benchmark uses Playwright's `storage_state` JSON (cookies + localStorage). browser-use needs an equivalent via CDP.

**Approach**: Inject cookies via `Network.setCookie` CDP call before task execution.

```python
async def inject_auth_cookies(
    session: BrowserSession,
    site: str,
    config: WebArenaEnvironment
) -> None:
    """
    Navigate to the site login page and authenticate,
    OR inject pre-captured cookies via CDP.
    """
```

**Two strategies** (implement both, prefer cookie injection):

1. **Cookie injection** (fast, deterministic):
   - Pre-capture auth cookies from each site into JSON files
   - Inject via `cdp_client.send.Network.setCookie(params=...)` before each task
   - Store in `benchmarks/webarena/auth_cookies/` (gitignored, generated by a setup script)

2. **Live login** (fallback):
   - Navigate to login page, fill credentials, submit
   - Use browser-use's own Agent with a simple login task
   - Slower but always works without pre-captured state

**Setup script**: `python -m benchmarks.webarena.auth --capture` logs into each site, captures cookies, saves JSON.

---

## Step 3: `benchmarks/webarena/adapter.py` — Response Format Adapter

Maps `AgentHistoryList` output → WebArena Verified's structured response format.

### WebArena Verified Response Schema

```python
class WebArenaAgentResponse(BaseModel):
    task_type: Literal["RETRIEVE", "MUTATE", "NAVIGATE"]
    status: Literal[
        "SUCCESS",
        "NOT_FOUND_ERROR",
        "PERMISSION_DENIED_ERROR",
        "ACTION_NOT_ALLOWED_ERROR",
        "DATA_VALIDATION_ERROR",
        "UNKNOWN_ERROR"
    ]
    retrieved_data: list[str] | None = None
    error_details: str | None = None
```

### Mapping Logic

```python
def adapt_history_to_response(
    history: AgentHistoryList,
    task: dict  # WebArena task definition
) -> WebArenaAgentResponse:
```

**`task_type`** — derived from the WebArena task's `eval.eval_types`:
- `string_match` → `RETRIEVE`
- `url_match` → `NAVIGATE`
- `program_html` (state-changing) → `MUTATE`

**`status`** — mapped from agent outcome:
- `history.is_successful() == True` → `SUCCESS`
- `history.is_successful() == False` and agent explicitly reported failure → map error text to appropriate status code
- `history.is_done() == False` (hit step limit) → `UNKNOWN_ERROR`

**`retrieved_data`** — from `history.final_result()`:
- Parse the agent's natural language response into a list of strings
- Split on commas, newlines, or numbered lists
- Strip whitespace, quotes, markdown formatting
- This is the most fragile part — may need task-type-specific parsing heuristics

---

## Step 4: HAR Recording — Fix HTTPS-Only Filter

**Critical issue**: browser-use's `HarRecordingWatchdog` filters to HTTPS-only requests (line 215 in `browser_use/browser/watchdogs/har_recording_watchdog.py`). WebArena containers serve on HTTP localhost. The HAR file will be **empty** unless this is fixed.

**Fix**: Add a `record_http` option to `BrowserProfile` (default `False` to preserve existing behavior):

```python
# In BrowserProfile
record_har_include_http: bool = False  # When True, also record HTTP requests in HAR

# In HarRecordingWatchdog._is_recordable()
if not self._is_https(url) and not self.profile.record_har_include_http:
    return False
```

**Alternative** (less invasive): The WebArena runner sets up the HAR watchdog with a monkey-patched filter. But a proper config option is cleaner and useful beyond WebArena.

**This is the only code change to browser-use core required by this plan.**

---

## Step 5: `benchmarks/webarena/runner.py` — WebArena Task Runner

**CLI:**
```
python -m benchmarks.webarena [--model MODEL] [--trials N] [--task-ids 0-50] [--sites shopping,reddit] [--output DIR]
```

**Flow per task:**

1. **Load task**: `wa.get_task(task_id)` via `webarena-verified` API
2. **Check site availability**: Verify required containers are running
3. **Reset environment** (if `task.require_reset`): `curl -X POST http://localhost:{port}/init`
4. **Resolve URLs**: Replace `__SHOPPING__` etc. in `start_url` with actual container URLs
5. **Set up browser session**:
   ```python
   har_path = output_dir / f"task_{task_id}" / "network.har"
   session = BrowserSession(BrowserProfile(
       headless=True,
       record_har_path=har_path,
       record_har_include_http=True,  # Required for localhost containers
   ))
   ```
6. **Inject auth cookies** (if `task.require_login`)
7. **Navigate to start_url**
8. **Run agent**:
   ```python
   agent = Agent(task=task["intent"], llm=llm, browser_session=session)
   history = await agent.run(max_steps=30)
   ```
9. **Extract metrics**: `metrics.extract_metrics(history)` (shared with Plan 1)
10. **Adapt response**: `adapter.adapt_history_to_response(history, task)` → save as `agent_response.json`
11. **Stop HAR recording**: Ensure HAR file is flushed and complete
12. **Evaluate**:
    ```python
    result = wa.evaluate_task(
        task_id=task_id,
        agent_response=response_path,
        network_trace=har_path,
    )
    ```
13. **Record result**: score (0/1), evaluator details, metrics
14. **Cleanup**: `await session.kill()`

**Multi-trial**: Repeat steps 3–14 for N trials. Reset environment between trials for MUTATE tasks.

**Parallelism**: Tasks within the same site run sequentially (shared state). Tasks across different sites can run in parallel via asyncio semaphore per site.

---

## Step 6: `benchmarks/webarena/evaluator.py` — Evaluation Wrapper

Thin wrapper around `webarena-verified`'s Python API:

```python
from webarena_verified.api import WebArenaVerified
from webarena_verified.types.config import WebArenaVerifiedConfig

class WebArenaEvaluator:
    def __init__(self, config: WebArenaEnvironment):
        self.wa = WebArenaVerified(WebArenaVerifiedConfig(
            environments=self._build_env_config(config)
        ))

    async def evaluate(self, task_id: int, response_path: Path, har_path: Path) -> EvalResult:
        result = self.wa.evaluate_task(
            task_id=task_id,
            agent_response=response_path,
            network_trace=har_path,
        )
        return EvalResult(
            task_id=task_id,
            score=result.score,        # 0 or 1
            status=result.status,
            evaluator_details=result,   # Full breakdown
        )

    def get_task(self, task_id: int) -> dict:
        return self.wa.get_task(task_id)

    def get_all_tasks(self) -> list[dict]:
        # Or load from HuggingFace: load_dataset("AmineHA/WebArena-Verified")
        return [self.wa.get_task(i) for i in range(812)]
```

**Offline re-evaluation**: Since evaluation only needs `agent_response.json` + `network.har`, results from previous runs can be re-evaluated without re-running the agent. The runner saves all artifacts to `output_dir/task_{id}/`.

---

## Step 7: `benchmarks/webarena/report.py` — WebArena-Specific Reporting

Extends Plan 1's report format with WebArena-specific dimensions:

**Per-site breakdown:**
```
## WebArena Verified Report — 2026-02-16
Model: gpt-4o-mini | Trials: 3 | Tasks: 812

| Site             | Tasks | Pass Rate (95% CI)     | Avg Steps | Avg Tokens | Avg Cost  |
|------------------|-------|------------------------|-----------|------------|-----------|
| Shopping         | 186   | 42.3% (38.1–46.5%)    | 8.2       | 24,500     | $0.0061   |
| Shopping Admin   | 128   | 38.7% (33.2–44.2%)    | 9.5       | 28,300     | $0.0071   |
| Reddit           | 204   | 51.2% (46.8–55.6%)    | 6.7       | 18,900     | $0.0047   |
| GitLab           | 148   | 35.1% (30.0–40.2%)    | 10.1      | 31,200     | $0.0078   |
| Wikipedia        | 94    | 58.5% (51.3–65.7%)    | 5.4       | 15,600     | $0.0039   |
| Map              | 52    | 44.2% (35.8–52.6%)    | 7.8       | 22,100     | $0.0055   |

### Aggregate (Template-Level Macro Average)
Overall: 44.2% (41.8–46.6%) | 7.9 avg steps | 23,400 avg tokens | $0.0059 avg cost
```

**Template-level macro averaging**: Tasks sharing the same `intent_template` form a group. Compute per-template success rate first, then average across templates. This prevents templates with many instantiations from dominating the score.

**95% confidence intervals**: Bootstrap resampling (N=1000) over template-level scores.

**Failure-mode breakdown** (from WebArena Verified's structured status codes):
```
### Failure Modes
SUCCESS: 362 (44.6%) | NOT_FOUND_ERROR: 89 (10.9%) | UNKNOWN_ERROR: 261 (32.1%)
PERMISSION_DENIED_ERROR: 45 (5.5%) | DATA_VALIDATION_ERROR: 32 (3.9%)
ACTION_NOT_ALLOWED_ERROR: 23 (2.8%)
```

**Baseline comparison**: Same mechanism as Plan 1 — store per-site and aggregate metrics, compute deltas.

---

## Implementation Order

1. `benchmarks/webarena/config.py` — environment config, no dependencies
2. **Fix HAR HTTPS-only filter** in `browser_use/browser/watchdogs/har_recording_watchdog.py` — the one core code change
3. `benchmarks/webarena/auth.py` — CDP cookie injection
4. `benchmarks/webarena/adapter.py` — response format mapping
5. `benchmarks/webarena/evaluator.py` — wrap webarena-verified API
6. `benchmarks/webarena/runner.py` — ties everything together
7. `benchmarks/webarena/report.py` — WebArena-specific reporting
8. Validation: run 10 Shopping tasks, verify HAR recording + evaluation + report

---

## Key Integration Risks

### 1. Response Format Adaptation (Medium Risk)
The agent's natural language `final_result()` must be parsed into `retrieved_data: list[str]`. This is inherently lossy — the agent might say "The best-selling product is Quest Lumaflex Band" and we need to extract `["Quest Lumaflex Band"]`. Mitigation: task-type-specific parsing heuristics + an optional LLM extraction step for ambiguous responses.

### 2. HAR Completeness (Low Risk)
The existing `HarRecordingWatchdog` is production-grade (HAR 1.2 format, captures request/response bodies). The only issue is the HTTPS-only filter — a straightforward config addition.

### 3. Authentication State (Low Risk)
Cookie injection via CDP is well-understood. The `BrowserSession` already supports `cdp_client.send.Network.setCookie`. Pre-captured cookie files are deterministic.

### 4. Environment Reset Timing (Medium Risk)
MUTATE tasks modify backend state. If the env-ctrl reset doesn't complete before the next trial, results will be wrong. Mitigation: poll env-ctrl status endpoint until reset confirms, with timeout.

### 5. Resource Requirements (Medium Risk)
Running all 6 Docker containers needs 16+ GB RAM. GitLab alone is heavy. Mitigation: support `--sites` flag to run subsets; start with Shopping + Wikipedia (lightest).

---

## Validation Criteria

1. `uv add webarena-verified` installs without conflicts
2. Docker containers start and serve web apps on expected ports
3. HAR recording captures HTTP (not just HTTPS) traffic from containers
4. Auth cookies injected via CDP enable authenticated task execution
5. `adapt_history_to_response()` produces valid `WebArenaAgentResponse` JSON
6. `wa.evaluate_task()` returns scores for at least 10 tasks without errors
7. Report includes per-site breakdown with confidence intervals
8. Results are reproducible: same task + same model → same score across runs
9. Offline re-evaluation works: saved artifacts can be re-scored without re-running
10. `tests/ci` suite still passes (HAR filter change is backward-compatible)

---

## Relationship to Plan 1

| Dimension | Plan 1 (Internal) | Plan 2 (WebArena Verified) |
|-----------|-------------------|---------------------------|
| **Purpose** | Internal regression detection | External comparability |
| **Tasks** | 7 custom fixtures (incl. invoice PDFs) | 812 verified, externally audited |
| **Environment** | pytest-httpserver (in-process) | Docker containers (self-hosted) |
| **LLM** | Mock or real | Real only (mock defeats the purpose) |
| **Evaluation** | Programmatic string matching | Deterministic type-aware comparators + backend state verification |
| **Cost per run** | Free (mock) to ~$0.05 (real) | ~$5–50 depending on model + task count |
| **Runtime** | Minutes | Hours (812 tasks × 30 steps each) |
| **Shared code** | `metrics.py`, `baseline.py` | Same |
| **Core changes** | None | HAR HTTPS filter option |

Both plans feed into the same baseline/report infrastructure. Plan 1 runs daily for regression detection. Plan 2 runs periodically (weekly/per-release) for external benchmarking.
